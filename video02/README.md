# python-data-analysis
Python数据分析

参考视频：
吴恩达-机器学习

1-绪论：初识机器学习
    机器学习
    监督学习：数据集中的每个样本都有相应的“正确答案”，再根据这些样本做出预测。
        回归问题、分类问题等
        例：垃圾邮件处理、癌症判断
    无监督学习：数据集中没有任何的标签或者是有相同的标签，已知数据集不知如何处理。
        聚类应用等
        例：新闻事件推荐
    鸡尾酒会问题：音频分离与合成
    demo-cocktail party problem algorithm
        Octave编程环境代码：
            [W,s,v]=svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');

2-单变量线性回归
    模型概述：
        监督学习：房价预测、肿瘤判断等
        单变量线性回归：只含有一个特征/输入变量
            数据集->学习算法拟合数据->构建模型预测数据

    代价函数概念
        数学定义
        训练集->选择参数->
            选择的参数决定了输出结果对于训练集、模型与实际值之间的差距，即建模误差。
            目标是选择出可以使得建模误差的平方和能够最小的模型参数
        代价函数（平方误差函数、平方误差代价函数）
    代价函数的直观理解
        三维空间展示、等高线图等
    梯度下降：用来求函数最小值
        思想：开始时随机选择一个参数的组合，计算代价函数，然后寻找下一个能让代价函数值下降最多的参数组合。
            持续这么做直到得到一个局部最小值。因为没有尝试完所有组合，所以不能确定是全局最小值。
        学习率
        参数更新：同时更新、同步更新
    梯度下降的直观理解
        接近局部最低，导数自动变小，梯度下降自动采取较小的幅度。
    平方误差函数结合梯度下降法以及平方代价函数=线性回归算法
    梯度下降的线性回归
        对于之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数。

3-线性代数回顾
    矩阵和向量
        矩阵维数即行数*列数
        矩阵元素（矩阵项）
        向量是一种特殊的矩阵
    加法和标量乘法
        矩阵加法：必须行列数相等，对应元素相加。
        矩阵乘法：每个元素都要乘。
        组合算法也类似
    矩阵向量乘法
        m*n矩阵乘以n*1向量，得到m*1向量
    矩阵乘法
        m*n矩阵乘以n*o矩阵，变成m*o矩阵
    矩阵乘法性质

    逆、转置

4-安装配置Matlab及Octave


5-多变量线性回归
    多维特征
        例：房价模型
            对模型增加更多的特征，构成一个含有多变量的模型。
            n代表特征数量，i个训练实例，构建特征矩阵（n+1个参数和n个变量，为了简化所以引入x_0=1）。
            此时参数是一个n+1维向量，任一训练实例也是n+1维向量。

    多变量梯度下降
        与单变量线性回归类似，在多变量线性回归也构建一个代价函数，则这个代价函数是所有建模误差的平方和。
        目标是找出使得代价函数最小的一系列参数
        随机选择一系列参数值，计算所有预测结果后再给所有参数一个新的值，如此循环直到收敛。

    梯度下降实践-特征缩放
        例；假设使用房屋尺寸（0-2000平方英尺）和房屋数量（0-5）两个特征，绘制代价函数的等高线图。
            图像会显得很扁，梯度下降需要非常多的迭代次数才能收敛。
            解决方法是尝试将所有特征的尺度都尽量缩放到-1到1之间

    梯度下降实践-学习率
        梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。
        存在一些自动测试是否收敛的方法但是没有图像直观，例：将代价函数的变化值与某个阈值进行比较
        梯度下降算法每次迭代受学习率的影响
            如果学习率过小，则达到收敛所需的迭代次数会非常高；
            如果学习率过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。
            可以考虑尝试使用经典学习率；0.01，0.03,0.1,0.3,1,3,10···

    特征和多项式回归
        线性回归并不适用于所有数据，有时需要曲线来适应数据，比如二次方模型、三次方模型等
        通常需要先观察数据然后尝试模型。另外可将多次方模型转化为线性回归模型
        注：如果采用多项式回归模型，在运行梯度下降算法前，特征缩放很有必要。

    正规方程
        对于某些线性回归问题，正规方程方法比梯度下降算法更好用。
        Octave中，正规方程写作：pinv(X'*X')*X'*y
        对于不可逆矩阵，正规方程是不可用的。

    梯度下降与正规方程比较：
        梯度下降：需要学习率、需要多次迭代、特征数量多时同样适用、适用各种类型的模型。
        正规方程：不需学习率、不需多次迭代、特征数量多时不适用、只适用线性模型。

    正规方程及不可逆性
        pinv()  伪逆
        inv()   逆

6-Octave教程
    基本操作
        Octave，一种原始语言，可快速实现算法原型。
            基本数学运算
            逻辑运算
            异或运算
            变量运算
            复杂屏幕输出：DISP命令
            向量和矩阵生成及赋值
            hist()命令绘制直方图
            help命令

    移动数据
        构建矩阵
        size()命令返回尺寸
        length()返回最大维度
        pwd命令显示工作路径
        who显示所有变量、whos
        load XXX.dat 加载文件
        save hello.mat v 存入硬盘 save hello.txt v -ascii
        clear 清除所有变量
        A[:,2] 取矩阵A第二列
        A=[A,[100,101,102]] 矩阵A加一列
        C=[A,B] 合并A、B  C=[A;B]

    计算数据
        点乘运算 A.*B
        A.^2 每个元素平方
        1./A 每个元素取倒数
        log(v) 对数运算
        exp(v) 幂次运算
        abs 求绝对值
        A' 转置
        A=magic(3) 魔方阵（幻方）：它们所有的行和列和对角线加起来都等于相同的值。
        [r,c]=find(A>=7) 查找A矩阵中大于等于7的元素
        sum(a) 求和

    数据绘制（绘图数据）
        t=[0:0.01:0.98];
        t
        y1=sin(2*pi*4*t)
        plot(t,y1)

        print -dpng 'myplot.png'
        Clf 清除一副图像

        可视化矩阵 imagesc(A)
            imagesc(magic(15)), colorbar, colormap gray

    控制语句：for、while、if语句
        for循环
            可用：break、continue
        while循环

        if-else语句

        定义函数
            function y = squareThisNumber(x)

    矢量|向量化
        使程序运行更快

    编程作业
        源码提交

7-逻辑回归（Logistic Regression）
    分类问题
        在分类问题中，要预测的变量y是离散的值。
            例：判断邮件是否垃圾邮件、判断一次金融交易是否是欺诈、肿瘤分类等
        线性回归算法不太适合解决分类问题，所以引入逻辑回归算法。
        逻辑回归算法实际上是一种分类算法，它适用于标签y取值离散的情况。

    假设陈述
        假设函数的表达式
            根据线性回归只能预测连续的值，然而对于分类问题需要输出0和1。
        逻辑回归模型的假设
            对于给定的输入变量，根据选择的参数计算输出变量=1的可能性。

    决策界限
        模型分界线，将预测为1的区域和预测为0的区域分隔开。
        可以用非常复杂的模型来适应非常复杂形状的判定边界

    代价函数
        定义用来拟合逻辑回归模型的参数的优化目标叫代价函数
        即监督学习问题中的逻辑回归模型的拟合问题
        对于线性回归模型，定义的代价函数是所有模型误差的平方和。
            理论上可以对逻辑回归模型沿用这个定义，但是代入之后得到的代价函数是一个非凸函数，
            这就意味着代价函数有许多局部最小值，将影响梯度下降算法寻找全局最小值，所以需重新定义代价函数。
        重新定义的单训练样本的代价函数是一个凸优化问题，即凸函数，并且没有局部最优值。
        一些梯度下降算法之外的选择：
            共轭梯度(Conjugate Gradient)
            局部优化法(Broyden fletcher goldfarb shann,BFGS)
            有限内存局部优化法(LBFGS)
            fminunc是matlab和octave中都带的一个最小值优化函数
            这些算法更加复杂和优越，通常不需要人工选择学习率而且比梯度下降算法更加快速。

    简化代价函数与梯度下降
        更新参数的规则

        另：若特征范围差距很大，那么应用特征缩放，同样可以让逻辑回归中梯度下降收敛更快。

    高级优化
        最小化代价函数可用共轭梯度法BFGS(变尺度法)、L-BFGS(限制变尺度法)
        最好直接使用Octave或Matlab的库
        当有一个很大的机器学习问题时，可以选择高级算法，而不是梯度下降。

    多元分类：一对多
        例：一个训练集有三个类别，将其分成三个二元分类问题，运行所有分类器，对每个输入变量都选择最高可能性的输出变量。
            训练这个逻辑回归分类器，输入新的x值做预测，选择一个让函数最大的结果。
        选择出哪一个分类器是可信度最高效果最好的，即可认为得到一个正确的分类。

8-正则化
    过拟合的问题










9-神经网络学习：表述








10-神经网络的学习








11-应用机器学习的建议






12-机器学习系统的设计










13-支持向量机









14-聚类











15-降维








16-异常检测









17-推荐系统











18-大规模机器学习








19-应用实例：图片文字识别








20-总结


































